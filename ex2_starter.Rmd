---
title: "Exercise 2-Group 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(arrow)
```

## Load data

Load the following data: + applications from `app_data_sample.parquet` + edges from `edges_sample.csv`

```{r load-data}
# import data
data_path <- "/Users/jessica.song/Desktop/MG/!671 Talent Analytics/Excercise2/"
applications <- read_feather(paste0(data_path,"app_data_starter.feather"))

applications
```

## Get gender for examiners

We'll get gender based on the first name of the examiner, which is recorded in the field `examiner_name_first`. We'll use library `gender` for that, relying on a modified version of their own [example](https://cran.r-project.org/web/packages/gender/vignettes/predicting-gender.html).

Note that there are over 2 million records in the applications table -- that's because there are many records for each examiner, as many as the number of applications that examiner worked on during this time frame. Our first step therefore is to get all *unique* names in a separate list `examiner_names`. We will then guess gender for each one and will join this table back to the original dataset. So, let's get names without repetition:

```{r}
# install.packages("genderdata")
install.packages("/Users/jessica.song/Downloads/genderdata-master", repos = NULL, type="source")

```


```{r gender-1}
library(gender)
#install_genderdata_package() # only run this line the first time you use the package, to get data for it

# get a list of first names without repetitions
examiner_names <- applications %>% 
  distinct(examiner_name_first)

examiner_names
```

Now let's use function `gender()` as shown in the example for the package to attach a gender and probability to each name and put the results into the table `examiner_names_gender`

```{r gender-2}
# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )

examiner_names_gender
```

Finally, let's join that table back to our original applications data and discard the temporary tables we have just created to reduce clutter in our environment.

```{r gender-3}
# remove extra colums from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)

# joining gender back to the dataset
applications <- applications %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")

# cleaning up
rm(examiner_names)
rm(examiner_names_gender)
gc()

```

## Guess the examiner's race

We'll now use package `wru` to estimate likely race of an examiner. Just like with gender, we'll get a list of unique names first, only now we are using surnames.

```{r race-1}
library(wru)

examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()

examiner_surnames
```

We'll follow the instructions for the package outlined here <https://github.com/kosukeimai/wru>.

```{r race-2}
examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()

examiner_race
```

As you can see, we get probabilities across five broad US Census categories: white, black, Hispanic, Asian and other. (Some of you may correctly point out that Hispanic is not a race category in the US Census, but these are the limitations of this package.)

Our final step here is to pick the race category that has the highest probability for each last name and then join the table back to the main applications table. See this example for comparing values across columns: <https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/>. And this one for `case_when()` function: <https://dplyr.tidyverse.org/reference/case_when.html>.

```{r race-3}
examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "Asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "Hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))

examiner_race
```

Let's join the data back to the applications table.

```{r race-4}
# removing extra columns
examiner_race <- examiner_race %>% 
  select(surname,race)

applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))

rm(examiner_race)
rm(examiner_surnames)
gc()
```

## Examiner's tenure

To figure out the timespan for which we observe each examiner in the applications data, let's find the first and the last observed date for each examiner. We'll first get examiner IDs and application dates in a separate table, for ease of manipulation. We'll keep examiner ID (the field `examiner_id`), and earliest and latest dates for each application (`filing_date` and `appl_status_date` respectively). We'll use functions in package `lubridate` to work with date and time values.

```{r tenure-1}
library(lubridate) # to work with dates

examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 

examiner_dates
```

The dates look inconsistent in terms of formatting. Let's make them consistent. We'll create new variables `start_date` and `end_date`.

```{r tenure-2}
examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))
```

Let's now identify the earliest and the latest date for each examiner and calculate the difference in days, which is their tenure in the organization.

```{r tenure-3}
examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)

examiner_dates
```

Joining back to the applications data.

```{r tenure-4}
applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")

rm(examiner_dates)
gc()
```

Save file as processed variables, to skip these steps in the following exercises.

```{r save-file}
write_feather(applications, paste0(data_path,"app_data_starter_coded.feather"))
```

# Rest of the exercise
```{r}
# import data
data_path <- "/Users/jessica.song/Desktop/MG/!671 Talent Analytics/Excercise2/"
applications1 <- read_feather(paste0(data_path,"app_data_starter_coded.feather"))

applications1
```

...
```{r}
# Create quarter columns
applications1$filing_quarter <- paste(year(applications1$filing_date), quarter(applications1$filing_date), sep="-Q")
applications1$abandon_quarter <- paste(year(applications1$abandon_date), quarter(applications1$abandon_date), sep="-Q")
applications1$issue_quarter <- paste(year(applications1$patent_issue_date), quarter(applications1$patent_issue_date), sep="-Q")

applications1

```

```{r}
# Summarize application number by examiner and quarter
# Aggregate data
aggregated_data <- applications1 %>%
  group_by(examiner_id, filing_quarter) %>%
  summarise(
    new_applications = n_distinct(application_number),
    .groups = "drop"
  )

# Aggregate and join for abandoned applications
abandoned_data <- applications1 %>%
  filter(!is.na(abandon_date)) %>%
  group_by(examiner_id, abandon_quarter) %>%
  summarise(
    abandoned_applications = n_distinct(application_number),
    .groups = "drop"
  )
aggregated_data <- left_join(aggregated_data, abandoned_data, by = c("examiner_id", "filing_quarter" = "abandon_quarter"))

# Aggregate and join for allowed applications
allowed_data <- applications1 %>%
  filter(!is.na(patent_issue_date)) %>%
  group_by(examiner_id, issue_quarter) %>%
  summarise(
    allowed_applications = n_distinct(application_number),
    .groups = "drop"
  )
aggregated_data <- left_join(aggregated_data, allowed_data, by = c("examiner_id", "filing_quarter" = "issue_quarter"))

# Calculate in-process applications
applications1$in_process_indicator <- with(applications1, 
  (filing_quarter <= filing_quarter) & 
  (
    (is.na(abandon_date) & is.na(patent_issue_date)) | 
    (!is.na(abandon_date) & abandon_quarter > filing_quarter) |
    (!is.na(patent_issue_date) & issue_quarter > filing_quarter) |
    (!is.na(abandon_date) & !is.na(patent_issue_date) & abandon_quarter > filing_quarter & issue_quarter > filing_quarter)
  )
)


in_process_data <- applications1 %>%
  group_by(examiner_id, filing_quarter) %>%
  summarise(
    in_process_applications = sum(in_process_indicator, na.rm = TRUE),
    .groups = "drop"
  )


aggregated_data <- left_join(aggregated_data, in_process_data, by = c("examiner_id", "filing_quarter"))

# Replace NA with 0 for counts
aggregated_data[is.na(aggregated_data)] <- 0

library(dplyr)

# Rename the column name of "filing_quarter"
aggregated_data <- aggregated_data %>%
  rename(analysis_quarter = filing_quarter)

aggregated_data

```

```{r}
# Summarize by art unit
# Convert appl_status_date to Date-Time Type
applications1$appl_status_date <- as.POSIXct(applications1$appl_status_date, format="%d%b%Y %H:%M:%S")

# Find the latest appl_status_date for each examiner_id and extract examiner_art_unit
latest_art_unit <- applications1 %>%
  group_by(examiner_id) %>%
  filter(appl_status_date == max(appl_status_date, na.rm = TRUE)) %>%
  summarise(current_art_unit = first(examiner_art_unit), .groups = "drop")

# Add current_art_unit column to applications1
applications1 <- left_join(applications1, latest_art_unit, by = "examiner_id")

applications1


# 1. Summarize the number of people in each art unit
people_in_art_unit <- applications1 %>%
  group_by(current_art_unit) %>%
  summarise(number_of_people = n_distinct(examiner_id)) %>%
  ungroup()  

# 2. Summarize the number of women in each art unit
women_in_art_unit <- applications1 %>%
  filter(gender.y == "female") %>% 
  group_by(current_art_unit) %>%
  summarise(number_of_women = n_distinct(examiner_id)) %>%
  ungroup()

# 3. Summarize the number of examiners by race in each art unit
examiners_by_race_in_art_unit <- applications1 %>%
  group_by(current_art_unit, race.y) %>%
  summarise(number_of_examiners = n_distinct(examiner_id)) %>%
  ungroup()

# View the results
print(people_in_art_unit)
print(women_in_art_unit)
print(examiners_by_race_in_art_unit)


```

```{r}
# Separation indicator
# Start by assuming all examiners are separated
aggregated_data$separation_indicator <- TRUE

# Create a subset of data for the quarter "2017-Q2"
data_2017_Q2 <- aggregated_data %>%
  filter(analysis_quarter == "2017-Q2") %>%
  group_by(examiner_id) %>%
  summarise(
    total_applications = sum(new_applications + abandoned_applications + allowed_applications + in_process_applications),
    separation_indicator = total_applications == 0, # Determine separation_indicator based on activity in "2017-Q2"
    .groups = "drop"
  )

# Get the examiner_ids that have separation_indicator as FALSE (meaning they had activity in "2017-Q2")
active_examiner_ids <- data_2017_Q2 %>%
  filter(separation_indicator == FALSE) %>%
  pull(examiner_id)

# Update the separation_indicator in the original aggregated_data
# Set separation_indicator to FALSE for examiners who had activity in "2017-Q2"
aggregated_data$separation_indicator[aggregated_data$examiner_id %in% active_examiner_ids] <- FALSE

# View the updated dataset
print(aggregated_data)

```

```{r}

# AU move indicator

# Group by examiner_id and check if there is more than one unique examiner_art_unit.x
applications1 <- applications1 %>%
  group_by(examiner_id) %>%
  mutate(
    AU_move_indicator = n_distinct(examiner_art_unit) > 1
  ) %>%
  ungroup()  # Remove the grouping

```

```{r}
# Descriptive table
# install.packages(gtsummary)
library(gtsummary)

# For aggregated_data: Create a descriptive table for separation_indicator
table_separation <- aggregated_data %>%
  distinct(examiner_id, .keep_all = TRUE) %>%
  select(separation_indicator) %>%
  tbl_summary(missing = "no") %>%
  add_n() %>%
  modify_header(label = "**Variable**")

# For applications1: Create a descriptive table for AU_move_indicator
table_au_move <- applications1 %>%
  distinct(examiner_id, .keep_all = TRUE) %>%
  select(AU_move_indicator) %>%
  tbl_summary(missing = "no") %>%
  add_n() %>%
  modify_header(label = "**Variable**")

# Estimate Turnover Rate, ensuring each examiner_id is counted only once
turnover_rate <- aggregated_data %>%
  distinct(examiner_id, .keep_all = TRUE) %>%
  summarise(
    total = n(),
    separated = sum(separation_indicator, na.rm = TRUE)
  ) %>%
  mutate(turnover_rate = separated / total)

# Estimate Mobility Rate, ensuring each examiner_id is counted only once
mobility_rate <- applications1 %>%
  distinct(examiner_id, .keep_all = TRUE) %>%
  summarise(
    total = n(),
    moved_au = sum(AU_move_indicator, na.rm = TRUE)
  ) %>%
  mutate(mobility_rate = moved_au / total)

# Display the tables and rates
table_separation
table_au_move
print(paste("Turnover Rate:", turnover_rate$turnover_rate))
print(paste("Mobility Rate:", mobility_rate$mobility_rate))


```

